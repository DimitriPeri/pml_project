---
title: 'PML course project - Qualitative activity recognition:'
author: "dnp"
date: "Friday, March 20, 2015"
output: html_document
---

Executive summary
---

In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, that were asked to perform barbell lifts correctly and incorrectly in 5 different ways, in order to predict the manner in which they did the exercise. This is the **classe** variable in the training set.

We train a predictive model with the **Random Forest** method and **Repeated Cross Validation** and use it to predict 20 different test cases. 

Loading and cleaning data
---

We clean the dataset from features that are:
- irrelevant to our predictive task (first 8 columns)
- predominantly NAs
- highly correlated
    
### Load data sets
 
```{r}
validset <- read.csv("pml-testing.csv")
dt <- read.csv("pml-training.csv", na.strings = "#DIV/0!")
dim(dt)
```

### Check and filter irrelevant featuress

```{r}
dt <- dt[, 8:length(dt)] # date, timestamp, etc
```

### Check and filter features with NAs

```{r}
classes <- sapply(dt, class)
table(classes) # check classes of variables
index_logical <- which(classes == "logical")
index_factor <- which(classes == "factor")
check_logical <- sapply(dt[ ,index_logical], summary) # check for NAs
check_factor <- sapply(dt[ ,index_factor], summary) # check for NAs
index_factor <- index_factor[-68] # classe variable should be factor 
dt <- dt[, -c(index_logical, index_factor)] # delete NA variables
na_cols <- sapply(dt, anyNA) #check more columns for NA values
index_na <- which(na_cols)
check_NAs <- sapply(dt[ ,index_na], summary)
dt <- dt[, -index_na] # delete more NA variables
```

### Check and filter correlated features

```{r}
library(caret)
descrCor <- cor(dt[, -53])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
```

### Final clean dataset

```{r}
fdt <- dt[, -highlyCorDescr]
dim(fdt)
```

Built predictive model
---

Because we have a large dataset, we will built a Random Forest model, which is characterised by its high predictive accuracy and minimal parameter tuning. In summary, we devide the data into a training and a testing set, fit the model with the training set and evaluate performance with the testing set.

```{r}
library(parallel)
library(doParallel)
```

### Data partition

```{r}
set.seed(222)
inTrain <- createDataPartition(y=fdt$classe, p=0.6, list=FALSE)
training <- fdt[inTrain,]
testing <- fdt[-inTrain,]
```

### Model fit

- **Random Forest**
- Train control method: **Repeated CV**
- **Repeats: 3**

```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 3)

registerDoParallel(clust <- makeCluster(detectCores()))
set.seed(222)
fit <- train(classe ~ ., method = "rf", data = training,
                 trControl = ctrl, tuneLength = 6)
stopCluster(clust)

fit

predictions <- predict(fit, newdata = testing)
```

Model evaluation
---

We assess our model's performance on the test set, using various statistics.

### Evaluation metrics

**Cross validation:**

```{r}
plot(fit)
```

Accuracy and error rates were estimated with **Repeated Cross-Validation**. Accuracy was used to select the optimal model using  the largest value. The final value used for the model was with 8 selected predictors.
    
**Accuracy and Out of sample error:**

```{r}
fit$finalModel
```

The above printout shows **the OOB estimate of error rate** and the class error rates of the model's fit.

**Confusion matrix and statistics:**

```{r}
confusionMatrix(predictions, testing$classe)
```

The overall **Accuracy rate** is `r round(confusionMatrix(predictions, testing$classe)$overall[[1]], 4) * 100`%. The **Null or No Information rate**, which is the largest proportion of the observed classes, is `r round(confusionMatrix(predictions, testing$classe)$overall[[5]], 4) * 100`%. The overall accuracy rate is greater than the rate of the largest class.

The **out of sample error rate** is `r 100 -  (round(confusionMatrix(predictions, testing$classe)$overall[[1]], 4) * 100)`%.

**Variable importance:**

```{r}
varImp(fit, scale = F)
plot(varImp(fit), top = 20)
```

Prediction
---

 Finally, we use our model to predict 20 different test cases.
 
```{r}
predict(fit, newdata = validset)
```

Conclusion
---

We trained a Random Forest predictive model with repeated cross-validation to our final 32-feature cleaned dataset. Our model has an accuracy on the testing set of `r round(confusionMatrix(predictions, testing$classe)$overall[[1]], 4) * 100`%. We use our model to predict the variable **classe** for 20 different test cases. Classe represents the manner in which they did the exercise.







dnp

    
 


